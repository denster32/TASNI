% Auto-generated by ml_ablation.py
% Feature importance table (Table B1)

\begin{deluxetable*}{lcc}
\tablecaption{Top-20 XGBoost Feature Importances\label{tbl:features}}
\tablehead{
  \colhead{Feature} & \colhead{Importance (Gain)} & \colhead{Rank}
}
\startdata
w2\_std & 0.5440 & 1 \\
w3\_w4\_color & 0.1529 & 2 \\
w1snr\_value & 0.0829 & 3 \\
w1mpro\_value & 0.0226 & 4 \\
w1\_n\_epochs & 0.0181 & 5 \\
w1\_mean & 0.0149 & 6 \\
w3mpro\_value & 0.0143 & 7 \\
w2snr\_value & 0.0141 & 8 \\
w4mpro\_value & 0.0126 & 9 \\
w3snr\_value & 0.0126 & 10 \\
w4snr\_value & 0.0109 & 11 \\
mag\_std & 0.0107 & 12 \\
pm\_total & 0.0102 & 13 \\
pmdec\_value & 0.0083 & 14 \\
pmra\_value & 0.0068 & 15 \\
mag\_range & 0.0065 & 16 \\
ecliptic\_lat & 0.0064 & 17 \\
w4sigmpro\_value & 0.0059 & 18 \\
w2\_w3\_color & 0.0056 & 19 \\
w1\_w2\_color\_original & 0.0055 & 20
\enddata
\tablecomments{Feature importances from XGBoost trained on all features to predict golden-sample membership. Gain-based importance measures the total improvement in the loss function attributed to each feature. Variability features (rms, std, range, epochs) collectively account for 57.7\% of total feature importance. Note that lightcurve-derived variability features are available only for the golden sample (Section~\ref{sec:variability}), so their discriminative power partly reflects information leakage rather than intrinsic physical differences (Section~\ref{sec:limitations}).}
\end{deluxetable*}

% Hyperparameter table (Table B2)

\begin{deluxetable*}{lll}
\tablecaption{Machine Learning Hyperparameter Settings\label{tbl:hyperparams}}
\tablehead{
  \colhead{Model} & \colhead{Parameter} & \colhead{Value}
}
\startdata
XGBoost & n\_estimators & 100 \\
XGBoost & max\_depth & 4 \\
XGBoost & learning\_rate & 0.1 \\
XGBoost & subsample & 0.8 \\
Isolation Forest & n\_estimators & 200 \\
Isolation Forest & contamination & 0.001 \\
Isolation Forest & max\_features & 0.8 \\
LightGBM & n\_estimators & 100 \\
LightGBM & max\_depth & 4 \\
LightGBM & learning\_rate & 0.05 \\
LightGBM & num\_leaves & 31 \\
Random Forest & n\_estimators & 100 \\
Random Forest & max\_depth & 6 \\
Random Forest & min\_samples\_leaf & 2
\enddata
\tablecomments{Hyperparameters used for each ranking model in the ablation study. XGBoost and LightGBM values match those used in the production pipeline (Section~\ref{sec:ml}). Isolation Forest parameters follow the contamination rate estimated from the WISE orphan catalog.}
\end{deluxetable*}
